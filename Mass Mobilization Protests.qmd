---
title-block-style: default
title-block-banner: "#191970"
title-block-banner-color: "#FFFFFF"
title: "Predicting Lethal Outcome of Protests"
author: "Alex Lee"
date: 2022-11-20
format:
   html:
    code-fold: true
    code-summary: "Show code"
    code-overflow: scroll
    code-line-numbers: true
    code-copy: hover
    code-tools: true
    theme: default
    toc: true
    toc-location: left
    
knitr:
  opts_chunk: 
    warning: false
    message: false 
    cache: true
    # column: page
    
bibliography: refs.bib
link-citations: true
---

This document attempts to predict government's response to mass mobilization protests- of which that may lead to lethal outcomes. "Lethal Outcome" in this project is defined as "Shooting" or "Killing". Random Forest Model is chosen at the end with AUC score of 0.87

## Libraries

```{r setup}

library(tidyverse)
library(tidymodels)
library(DT)
library(embed)
library(themis)
library(lubridate)
library(countrycode)
library(corrplot)
library(skimr)
library(GGally)
library(kaggler)
library(janitor)
library(wbstats)
library(vip)

```

## Data Import

Four distinct datasets will be explored in this document, where **first two** will be used in the final model .

1.  **df_protest**: Mass Mobillization Data of 166 countries, 1990-2020, tracking protests against governments. [@protest]
2.  **df_cpi**: Corruption Perception Index Data of 189 countries, 1998-2021. [@cpi]
3.  **df_gdp**: GDP Data of 217 countries, 1960-2021. [@gdp]
4.  **df_hap**: World Happiness Data of 164 countries, 2015-2022. [@hap]

```{r Data Import, results='hide'}

# Mass Mobilization Protest Data
df_protest <- read_csv("data/Mass Mobilization Protests/df_massmobil.csv")

# Corruption Perception Index Data
df_cpi <- read_csv("data/Mass Mobilization Protests/df_cpi.csv")

# GDP Data (Not used in final model)
df_gdp <- wb_data("NY.GDP.MKTP.CD") %>% select(iso_3 = iso3c, year = date, gdp = NY.GDP.MKTP.CD)

# World Happiness Index Data (Not used in final model)
get_kaggle_data <- function() {
  
    # Authenticate to Kaggle API 
    # This is Hidden in Github. Please Generate your own for reproducibility.
    kaggler::kgl_auth(creds_file = "kaggle.json")
    
    # Define starting point of download
    kaggle_start_date <- 2015
    
    # Create blank vector to store output from below logic
    kaggle_years <- c()
    
    # Get distinct years since 2015 up to current year
    while (kaggle_start_date <= Sys.Date() %>% year()) {
      kaggle_years <- append(kaggle_start_date, kaggle_years)
      kaggle_start_date <- kaggle_start_date + 1
    }
    
    # Get String values to use in download logic
    kaggle_years_csv <- kaggle_years %>% paste0(".csv")
    
    # Clear df_hp object in case it already exists
    if(exists("df_hp")) {rm(df_hp)}
    
    # Download data from Kaggle and store & append to df_hp object
    for (i in kaggle_years_csv) {
      
      # Download
      df_temp <- kgl_datasets_download(
        owner_dataset = "mathurinache/world-happiness-report",
        fileName = i,
        datasetVersionNumber = 2)
      
      # Data Cleansing
      df_temp <-
        lapply(df_temp, function(x) sub(",", ".", x)) %>%
        data.frame() %>%
        clean_names() %>%
        select(matches("country|score")) %>%
        rename_with( ~ paste0("country"), matches("country")) %>%
        mutate(year = sub(".csv", "", i) %>% as.numeric())
      
      # Assign df_temp to df_hp if first loop, append if loop > 1
      ifelse(exists("df_hp"),
             df_hp <- df_hp %>% bind_rows(df_temp),
             df_hp <- df_temp)
    }
  
  # Additional Cleansing
  df_hp <- df_hp %>% 
    
    # Concat different happiness score namings to one
    select(country, year, happiness_score, ladder_score, score) %>% 
    mutate(final_score = case_when(!is.na(happiness_score) ~ happiness_score,
                                   !is.na(ladder_score) ~ ladder_score,
                                   !is.na(score) ~ score,
                                   TRUE ~ NA_character_)) %>%
    mutate(final_score = as.numeric(final_score)) %>%
    
    # Get iso3 code from country names
    mutate(iso_3 = countrycode(country, "country.name", "iso3c",
                               custom_match = c("Kosovo" = "XXK",
                                                "Somaliland region" = "SOM",
                                                "Somaliland Region" = "SOM"))) %>%
    select(year, iso_3, happiness_score = final_score)
  
}

df_hap <- get_kaggle_data()

```

## Data Overview

#### 1. Mass Mobillization Data

```{r Data Overview Protest}

df_protest %>% skim() %>% datatable(options = list(scrollX = TRUE, dom = "lrtip"))
```

#### 2. Corruption Perception Index Data

```{r Data Overview cpi}

df_cpi %>% skim() %>% datatable(options = list(scrollX = TRUE, dom = "rti"))
```

#### 3. GDP Data

```{r Data Overview gdp}

df_gdp %>% skim() %>% datatable(options = list(scrollX = TRUE, dom = "rti"))
```

#### 4. World Happiness Data

```{r Data Overview hap}

df_hap %>% skim() %>% datatable(options = list(scrollX = TRUE, dom = "rti"))
```

## Data Transformation

Data Overview section gave us a pretty good idea of how the data is structured, and I'd like to restructure it a bit and complete my joins prior to proceeding with any exploratory analysis.

#### Data Cleansing

Here we'll apply the first layer of data cleansing to edit dirty data, create target label, and remove what we don't need. Additional Transformation will be done in the Feature Engineering section down below.

```{r Data Cleansing}
df_cleansed <- 
  df_protest %>% 
  
  # Remove records (years) that did not have any protests
  filter(protest != 0) %>%
  
  # I'm particularly interested in [Participants] since it may indicate how major the protest is; it seems like the feature is in mixed numerical/character format.
  # Manually reformat [Participants] to numerical values. 
  # Use the lower bound when the value is a range.
  mutate(participants = tolower(participants)) %>%
  mutate(participants = case_when(grepl("police put the turnout at 1,700", participants) ~ "1700", 
                                  grepl("tens of", participants) ~ "",
                                  grepl("several dozen", participants) ~ "12",
                                  grepl("several hundred", participants) ~ "100",
                                  grepl("several thousand", participants) ~ "1000",
                                  grepl("thousand", participants) ~ "1000",
                                  grepl("two million", participants) ~ "2000000",
                                  grepl("million", participants) ~ "1000000",
                                  grepl("6000 to 8000", participants) ~ "6000",
                                  grepl("3000 to 5000", participants) ~ "3000",
                                  grepl("2000 to 3,000", participants) ~ "2000",
                                  grepl("2000 to 3000", participants) ~ "2000",
                                  grepl("2,000 to 3,000", participants) ~ "2000",
                                  grepl("2,000 to 4,000 people", participants) ~ "2000",
                                  grepl("about 100 activists", participants) ~ "100",
                                  grepl("between 11000 and 45000", participants) ~ "11000",
                                  grepl("between 35,000 and 50,000", participants) ~ "35000",
                                  grepl("Dennis Kwok, said more than 2,000", participants) ~ "2000",
                                  grepl("200 kamnans and village heads", participants) ~ "200",
                                  TRUE ~ participants)) %>%
  mutate(participants = gsub("[-&;].*", "", participants)) %>%
  mutate(participants = gsub("[^0-9]", "", participants)) %>%
  mutate(participants = as.numeric(participants)) %>%
  
  # Government responses are dispersed into multiple features.
  # Concatenate all response columns into one.
  unite(col='concat_response', contains("stateresponse"), sep="_", remove=FALSE) %>%
  
  # Although I later determine to focus on the lethality of the response, here I try to explore the degree of danger level as a target label.
  # Identify danger level of government response toward protesters.
  mutate(danger_level = case_when(grepl("killing|shooting", concat_response) ~ 2,
                                  grepl("arrest|beating|crowd dispersal", concat_response) ~ 1,
                                  grepl("ignore|accomodation", concat_response) ~ 0,
                                  TRUE ~ 0) %>% as.factor()) %>%
  
  # Create target label to be used in model.
  # Identify whether government response was lethal to protesters.
  mutate(lethal = case_when(grepl("killing|shooting", concat_response) ~ "Lethal",
                            TRUE ~ "Non-Lethal") %>% factor(levels = c("Lethal", "Non-Lethal"))) %>%
  
  # Remove original government response cols since I don't need it anymore.
  select(-contains("response")) %>%
  
  # Manually One hot encode protester demand cols due to the way it's structured.
  mutate(n = 1) %>% 
  pivot_longer(cols = contains("demand")) %>%
  mutate(value = 
           case_when(
             is.na(value) ~ "None",
             value == "." ~ "None",
             TRUE ~ value)
         ) %>%
  select(-name) %>%
  distinct() %>%
  pivot_wider(names_from = value, values_from = n, values_fill = 0) %>%
  select(-None) %>%

  # Concat Year Month Day cols together.
  mutate(startdate = as.Date(paste(startyear, startmonth, startday, sep="-"),"%Y-%m-%d")) %>%
  mutate(enddate = as.Date(paste(endyear, endmonth, endday, sep="-"),"%Y-%m-%d")) %>%
  
  # Calculate duration of protests (in days).
  mutate(days_in_protest = as.numeric(enddate - startdate)) %>%
  
  # Standardize col names.
  janitor::clean_names()

```

#### Data Joining

Let's join all datasets together using iso_3 country code and year as mapping keys.

```{r Data Joining}

df_cleansed_joined <- 
  df_cleansed %>% 
  
  # Create iso_3 with "countrycode" package, which I'll use as a joining key.
  mutate(iso_3 = countrycode(country, "country.name", "iso3c",
                             custom_match = c("Czechoslovakia" = "CZE",
                                              "Germany East" = "DEU",
                                              "Kosovo" = "XXK",
                                              "Serbia and Montenegro" = "SRB",
                                              "Yugoslavia" = "YUG"))) %>%
  
  # Join Corruption Perception Index Score.
  left_join(df_cpi %>% 
              mutate(cpi_score =
                       case_when(Year <= 2009 ~ cpi_score * 10,
                                 TRUE ~ cpi_score)) %>%
              filter(!is.na(cpi_score)), 
            by = c("iso_3", "year" = "Year")) %>%
  
  
  # Join GDP.
  left_join(df_gdp, by = c("iso_3", "year")) %>%
  
  
  # Join Happiness Score.
  left_join(df_hap, by = c("iso_3", "year"))

  
```

## EDA

```{r Exploratory Analysis}
pairs <- ggpairs(df_cleansed_joined %>% 
                  select(lethal,
                         year,
                         protestnumber,
                         protesterviolence,
                         participants,
                         days_in_protest,
                         cpi_score,
                         gdp,
                         happiness_score
                         ))

```

## Correlation Analysis

Let's check correlation between features to understand which features might be repetitive.

```{r EDA}

# Create correlation matrix & plot.
df_col <- cor(df_cleansed_joined %>% 
                select_if(is.numeric) %>% 
                select(-protest, -id, -ccode), 
              method = "spearman", use = "complete.obs")
corrplot(df_col, "square", addrect = 3, order = 'hclust', diag = FALSE)

# Quick Significance Test on correlated variables.
cor.test(df_cleansed_joined$gdp, df_cleansed_joined$happiness_score)
cor.test(df_cleansed_joined$gdp, df_cleansed_joined$cpi_score)
cor.test(df_cleansed_joined$happiness_score, df_cleansed_joined$cpi_score)

```

## Feature Selection

A lot of time was spent on gathering the data, but we'll remove gdp and happiness data since the correlation test tells us that cpi, gdp, and happiness score are significantly correlated. I select cpi out of the three after observing how it impacts the final model score. I'll also remove all other highly correlated features.

```{r Feature Selection}
df_final <- 
  df_cleansed_joined %>% 
  select(lethal,
         id,
         year,
         startmonth,
         iso_3,
         protestnumber,
         protesterviolence,
         participants,
         days_in_protest,
         cpi_score,
         political_behavior_process,
         labor_wage_dispute,
         land_farm_issue,
         police_brutality,
         price_increases_tax_policy,
         social_restrictions,
         removal_of_politician
         ) %>%
  mutate_if(is.character, factor)
```

## Train Test Split

Split the data to 75% Train and 25% Test, and create 25 partitions of K-Folds.

```{r Train Test Split}
set.seed(80)
pt_split <- initial_split(df_final, strata = lethal)

pt_train <- training(pt_split)
pt_test <- testing(pt_split)

df_folds <- vfold_cv(pt_train, v = 25, strata = lethal)

```

## Feature Engineering

We'll use recipes package to create a recipe of feature engineering pipeline.

```{r Feature Engineering}
pt_rec <- recipe(lethal ~ ., data = pt_train) %>%
  update_role(id, new_role = "id") %>%

  # Impute & Bin Missing [Participants].
  step_impute_knn(participants) %>%
  step_discretize_xgb(participants, outcome = "lethal") %>%
  
  # Impute & Bin [CPI Score].
  step_impute_knn(cpi_score) %>%
  step_discretize_xgb(cpi_score, outcome = "lethal") %>%
   
  # Convert [Participants] and [CPI Score] to ordinal factor.
  step_mutate(participants = factor(participants, levels = sort(unique(participants)), ordered = TRUE)) %>%
  step_mutate(cpi_score = factor(cpi_score, levels = sort(unique(cpi_score)), ordered = TRUE)) %>%

  # Convert [Participants] and [CPI Score] to numeric.
  step_ordinalscore(c("participants", "cpi_score")) %>%
  
  # One Hot encode all nominal features.
  step_dummy(all_nominal(), -all_outcomes()) %>%
  
  # Remove features with no variability for safe measure.
  step_zv(all_numeric()) %>%
  
  # Normalize values of all predictors.
  step_normalize(all_predictors()) %>%
  
  # Downsample
  step_downsample(lethal) %>%
  
  prep()
```

## Model

Predicting lethality of a protest would be categorized into a classification problem. There are many algorithms to use for such cases, and we'll try to use couple of them and pick the one with best outcome in the end. We'll cover KNN, Random Forest, and XGBoost algorithms.

### Model: Weighted K-Nearest Neighbor

Here we create a specification for kknn, then use tune_grid to collect performance metrics of different hyper parameters.

```{r Weighted K-Nearest Neighbor 1}
kknn_spec <- 
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>%
  set_mode("classification") %>% 
  set_engine("kknn") 

kknn_workflow <- 
  workflow() %>% 
  add_recipe(pt_rec) %>% 
  add_model(kknn_spec) 

# Use the model if it already exists- if not, create the model.
kknn_path <- file.path("data/Mass Mobilization Protests/kknn_tune.rds")

if (file.exists(kknn_path)) {
  kknn_tune <- read_rds(file = kknn_path)
  } else {
    kknn_tune <-
    tune_grid(kknn_workflow, resamples = df_folds, grid = 11)
    saveRDS(kknn_tune, file = kknn_path)
  }
```

What are the best parameters according to our metrics collected from different folds?

```{r Weighted K-Nearest Neighbor 2}
# Show best parameters
show_best(kknn_tune, metric = "accuracy") %>% datatable(options = list(scrollX = TRUE, dom = "rti"))
show_best(kknn_tune, metric = "roc_auc") %>% datatable(options = list(scrollX = TRUE, dom = "rti"))

# Show all parameters from folds.
plt_kknn_neighbors <- 
  kknn_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, neighbors) %>%
  ggplot(aes(neighbors, mean)) +
  geom_point(show.legend = FALSE)

plt_kknn_weight_func <-
  kknn_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, weight_func) %>%
  ggplot(aes(weight_func, mean)) +
  geom_point(show.legend = FALSE)

grid.arrange(plt_kknn_neighbors, plt_kknn_weight_func)
```

Lastly, fitting the model

```{r Weighted K-Nearest Neighbor Fit}
# Create a workflow to use a model with best hyper parameters based on roc curve.
final_kknn_wf <- kknn_workflow %>%
  finalize_workflow(select_best(kknn_tune, "roc_auc"))

# Fit the model.
protest_fit_kknn <- last_fit(final_kknn_wf, pt_split)
```

We'll follow the same steps below to create Random Forest and XGBoost model.

### Model: Random Forest

```{r Random Forest 1}
ranger_spec <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_mode("classification") %>% 
  set_engine("ranger")

ranger_workflow <- 
  workflow() %>% 
  add_recipe(pt_rec) %>% 
  add_model(ranger_spec) 

# Use the model if it already exists- if not, create the model.
ranger_path <- file.path("data/Mass Mobilization Protests/rforest_tune.rds")

if (file.exists(ranger_path)) {
  ranger_tune <- read_rds(file = ranger_path)
  } else {
    ranger_tune <-
    tune_grid(ranger_workflow, resamples = df_folds, grid = 11)
    saveRDS(ranger_tune, file = ranger_path)
    }
```

```{r Random Forest 2}
# Show Best Parameters.
show_best(ranger_tune, metric = "accuracy") %>% datatable(options = list(scrollX = TRUE, dom = "rti"))
show_best(ranger_tune, metric = "roc_auc") %>% datatable(options = list(scrollX = TRUE, dom = "rti"))

# Show all parameters from folds.
ranger_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
               values_to = "value",
               names_to = "parameter") %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap( ~ parameter, scales = "free_x")
```

```{r Random Forest 3}
# Create a workflow to use a model with best hyper parameters based on roc curve.
final_rf_wf <- ranger_workflow %>%
  finalize_workflow(select_best(ranger_tune, "roc_auc"))

# Fit the model.
protest_fit_rf <- last_fit(final_rf_wf, pt_split)

```

### Model: XGBoost

```{r XGBoost 1}
xgboost_spec <- 
  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(),
             loss_reduction = tune(), sample_size = tune()) %>%
  set_mode("classification") %>% 
  set_engine("xgboost") 

xgboost_workflow <- 
  workflow() %>% 
  add_recipe(pt_rec) %>% 
  add_model(xgboost_spec) 

# Use the model if it already exists- if not, create the model.
xgboost_path <- file.path("data/Mass Mobilization Protests/xgb_tune.rds")

if (file.exists(xgboost_path)) {
  xgboost_tune <- read_rds(file = xgboost_path)
  } else {
    xgboost_tune <-
    tune_grid(xgboost_workflow, resamples = df_folds, grid = 11)
    saveRDS(xgboost_tune, file = xgboost_path)
    }
```

```{r XGBoost 2}
# Show Best Parameters.
show_best(xgboost_tune, metric = "accuracy") %>% datatable(options = list(scrollX = TRUE, dom = "rti"))
show_best(xgboost_tune, metric = "roc_auc") %>% datatable(options = list(scrollX = TRUE, dom = "rti"))

# Show all parameters from folds.
xgboost_tune %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, trees, min_n, tree_depth, learn_rate, loss_reduction, sample_size) %>%
  pivot_longer(trees:sample_size,
               values_to = "value",
               names_to = "parameter") %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap( ~ parameter, scales = "free_x")
```

```{r XGBoost 3}
# Create a workflow to use a model with best hyper parameters based on roc curve.
final_xgb_wf <- xgboost_workflow %>%
  finalize_workflow(select_best(xgboost_tune, metric = "roc_auc"))

# Fit the model.
protest_fit_xgb <- last_fit(final_xgb_wf, pt_split)

```

## Model Results

Let's take a look at accuracy and roc_auc metrics to understand which model performed the best.

```{r Results 1}
collect_metrics(protest_fit_kknn) %>% mutate(model = "KKNN")
collect_metrics(protest_fit_rf) %>% mutate(model = "Random Forest")
collect_metrics(protest_fit_xgb) %>% mutate(model = "XGBoost")
```

#### Accuracy: Confusion Matrix

```{r Results 2}
# Confusion Matrix
cm <-
  collect_predictions(protest_fit_kknn) %>% 
    mutate(model = "kknn") %>%
    bind_rows(collect_predictions(protest_fit_rf) %>%
                mutate(model = "rforest")) %>%
    bind_rows(collect_predictions(protest_fit_xgb)%>%
                mutate(model = "xgboost")) %>%
    group_by(model) %>%
    conf_mat(lethal, .pred_class)
 
kknn_cm <- cm %>% filter(model == "kknn") %>% .$conf_mat %>% .[[1]] %>% 
  autoplot(type = "heatmap") + ggtitle("KKNN") + theme(plot.title = element_text(hjust = 0.95))
rf_cm <- cm %>% filter(model == "rforest") %>% .$conf_mat %>% .[[1]] %>% 
  autoplot(type = "heatmap") + ggtitle("Random Forest") + theme(plot.title = element_text(hjust = 0.95))
xgboost_cm <- cm %>% filter(model == "xgboost") %>% .$conf_mat %>% .[[1]] %>% 
  autoplot(type = "heatmap") + ggtitle("XGBoost") + theme(plot.title = element_text(hjust = 0.95))

grid.arrange(kknn_cm, rf_cm, xgboost_cm)
```

#### ROC AUC: ROC Curve

```{r Results 3}
# ROC Curve
collect_predictions(protest_fit_kknn) %>%
  mutate(model = "KKNN") %>%
  bind_rows(collect_predictions(protest_fit_rf) %>%
              mutate(model = "Random Forest")) %>%
  bind_rows(collect_predictions(protest_fit_xgb)%>%
              mutate(model = "XGBoost")) %>%
  group_by(model) %>%
  roc_curve(lethal, .pred_Lethal) %>%
  autoplot()
```

We can clearly see that Random Forest Model is performing the best in both accuracy and roc_auc.

I especially like that it's performing at 0.87 roc_auc, potentially meaning that the model is good at generalization and will do pretty well at predicting with unseen data.

## RF: Variable Importance

What can we learn from the model? What are the most important features to predict lethality according to the model?

```{r Variable Importance}
imp_spec_rforest <- ranger_spec %>%
  finalize_model(select_best(ranger_tune, metric = "roc_auc")) %>%
  set_engine("ranger", importance = "permutation")

workflow() %>%
  add_recipe(pt_rec) %>%
  add_model(imp_spec_rforest) %>%
  fit(pt_train) %>%
  extract_fit_parsnip() %>%
  vip(aesthetics = list(alpha = 0.8, fill = "midnightblue"))

```

## Conclusion

Model tells us that 2 key features are the main predictors of life-threatening violence that may happen during protests.

1.  Protester Violence
2.  Corruption Perception Score

Our data does not indicate whether a protester violence was initiated by protesters or the government. However, it's hard to argue against the fact that provocation by either side may escalate the situation.

In addition, citizens' perception of government corruption and associated government track record leading up to the event also was a great predictor of how the government would respond.

This sounds like a no-brainer now that we look at it, but it's really nice to confirm it with real concrete data.

As a closing comment, this analysis in no way makes any statement about how the protest should be conducted, nor which method is the effective method to invoke change. Many prominent studies say that non-violent protest is the best method for social movements to pursue causes, but the reality is more complicated.

You can find all the resources of this document in my [Github Repo](https://github.com/ALEE0001/pp_protest_lethality_prediction)
